---
title: "Advanced AI for Healthcare"
author: "Dr. Greg Chism"
institute: U of A InfoSci + DataLab
subtitle: Introduction to LSTMs
title-slide-attributes:
  data-slide-number: none
format:
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    chalkboard: true
editor: visual
execute:
  freeze: auto
  echo: true
auto-stretch: false
footer: "[\U0001F517 ua-datalab.github.io](https://ua-datalab.github.io/)"
logo: "https://github.com/clizarraga-UAD7/DataScienceLab/raw/main/images/UADLSquareLogo.png?raw=true"
resource_files:
- slides.scss
---

## Objective {.smaller}

::: fragment
Provide a conceptual overview of sequence modeling, focusing on the principles of LSTMs and their applications in handling sequential healthcare data.
:::

::: fragment
> No code today ðŸ™‚
:::

## Overview of topics

::: incremental
1.  Introduction to Sequence Modeling

2.  LSTMs in Healthcare

3.  Why Use LSTMs?

4.  Key Components of LSTMs
:::

# Introduction to Sequence Modeling

## Introduction to Sequence Modeling {.smaller}

::: fragment
**Definition:** Sequence modeling involves predicting future values based on previous data points, capturing patterns and dependencies over time.
:::

::: incremental
-   **Examples of Sequential Data:**

    -   Time series (e.g., heart rate over time).

    -   Text (e.g., clinical notes).

    -   EHR data (e.g., patient vitals tracked over hospital stays).

-   **Importance in Healthcare:**

    -   Helps predict patient outcomes, monitor disease progression, and tailor treatments based on sequential patterns in patient data.
:::

## Recurrent Neural Networks (RNNs) {.smaller}

![](images/rnn-01.png){fig-align="center" width="660"}

::: {.incremental .small}
-   Handles sequence data, like time series or language.

-   Captures dependencies over time.

-   **Health Sciences Application:** Predicting patient outcomes over time.
:::

## RNNs (another view) {.smaller}

![](images/rnn-1.png){fig-align="center" width="848"}

::: incremental
-   X = input vector

-   A = hidden vector

-   h = output vector
:::

## Long Short-Term Memory (LSTM) {.smaller}

Widely because they retain context for extended periods.

::: columns
::: {.column width="50%"}
**Apple**

::: fragment
![](images/iphone-keyboard.webp){fig-align="center" width="174"}
:::

::: fragment
![](images/hey-siri.jpg){fig-align="center" width="457"}
:::
:::

::: {.column width="50%"}
**Google**

::: fragment
![](images/Alphago_logo.png){fig-align="center" width="762"}
:::

::: fragment
![](images/google-translate.webp){fig-align="center" width="356"}
:::
:::
:::

## LSTMs in healthcare {.smaller}

::: incremental
-   **Clinical Diagnosis & Prediction**: Predict patient outcomes, like disease progression or readmission, from EHR time-series.

-   **Drug Discovery**: Predict drug efficacy and side effects from molecular sequences.

-   **Medical Time-Series**: Detect abnormalities (e.g., arrhythmias, epilepsy) from physiological signals like ECG and EEG.

-   **Healthcare Speech Processing**: Improve voice recognition in medical transcription and virtual assistants.

-   **Patient Pathway Modeling**: Analyze clinical event sequences to predict future healthcare needs.

-   **Wearable Monitoring**: Monitor health in real time through wearable data (e.g., heart rate, sleep patterns).
:::

## Why use LSTMs? {.xsmall}

::: incremental
-   **Sequential Data Processing**: Ideal for analyzing time-series data like EHRs and vital signs.

-   **Capturing Long-Term Patterns**: Retains medical history for better predictions of outcomes and disease progression.

-   **Anomaly Detection**: Identifies irregularities in continuous data, such as ECG or blood pressure trends.

-   **Natural Language Understanding**: Enhances medical transcription and summarization of clinical notes.

-   **Handles Irregular Data**: Manages gaps and irregular intervals in patient records effectively.

-   **Real-Time Monitoring**: Powers wearable device data analysis for early alerts.

-   **End-to-End Learning**: Learns directly from raw healthcare data, minimizing manual preprocessing.
:::

## Long Short-Term Memory (LSTM)

![](images/lstm-1.png){fig-align="center" width="818"}

## Key components: Input state ($X_t$) {.smaller}

![](images/lstm-input-state.png){fig-align="center" width="715"}

::: fragment
**Input state** $X_t$**:** The feature vector for the current time step.
:::

## Key components: Cell state ($C_t$) {.smaller}

![](images/lstm-cell-state.png){fig-align="center" width="715"}

::: fragment
**Cell state** $C_t$**:** The internal memory, updated at each time step, storing long-term information.
:::

## Key components: Input gates (Ïƒ) {.smaller}

![](images/lstm-gates.png){fig-align="center" width="555"}

::: columns
::: {.column width="33.33%"}
::: fragment
**Forget Gate:** <br> Decides how much of the previous cell state $c_{t-1}$â€‹ to keep or forget.
:::
:::

::: {.column width="33.33%"}
::: fragment
**Input Gate:** <br> Controls how much of the new input will be added to the cell state.
:::
:::

::: {.column width="33.33%"}
::: fragment
**Output Gate:** Determines how much of the cell state influences the next hidden state $h_t$
:::
:::
:::

## Key components: Tanh layer {.smaller}

![](images/lstm-tanh-layer.png){fig-align="center" width="714"}

::: fragment
**Tanh (tanh)**: Creates candidate values for updating the cell state, with outputs between -1 and 1.
:::

## Key components: Multiplication (x) {.smaller}

![](images/lstm-multiplicative.png){fig-align="center" width="714"}

::: fragment
**Multiplication (x):** Element-wise multiplication used in forget, input, and output gates to adjust the cell and hidden states.
:::

## Key components: Addition (+) {.smaller}

![](images/lstm-additive.png){fig-align="center" width="714"}

::: fragment
**Addition (+):** Combines the results of the forget and input gates to update the cell state.
:::

## Key components: Copy & Concatenate {.smaller}

![](images/lstm-copy-concat.png){fig-align="center" width="658"}

::: fragment
**Copy or concatenate information:** helps retain information across time steps. The copy operation preserves the flow of important information.
:::


## Key components: Hidden state ($h_t$) {.smaller}

![](images/lstm-hidden-state.png){fig-align="center" width="712"}

::: fragment
**Hidden state** $h_t$**:** The output at time step $t$, filtered from the cell state and passed to the next cell.
:::

## Process flow

![](images/lstm-process-1.png){fig-align="center" width="791"}

## Process flow

![](images/lstm-process-2.png){fig-align="center" width="791"}

## Process flow

![](images/lstm-process-3.png){fig-align="center" width="791"}

## Process flow

![](images/lstm-process-4.png){fig-align="center" width="791"}

## Process flow

![](images/lstm-process-5.png){fig-align="center" width="799"}

## Process flow

![](images/lstm-process-6.png){fig-align="center" width="791"}

## Full picture

![](images/lstm-whole.png){fig-align="center" width="769"}

# Thank You ðŸ˜Š {style="text-align: center;"}
