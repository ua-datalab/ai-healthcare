---
title: "Advanced AI for Healthcare"
author: "Dr. Greg Chism"
institute: U of A InfoSci + DataLab
subtitle: Introduction to Deep Learning I
title-slide-attributes:
  data-slide-number: none
format:
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    chalkboard: true
editor: visual
execute:
  freeze: auto
  echo: true
auto-stretch: false
footer: "[\U0001F517 ua-datalab.github.io](https://ua-datalab.github.io/)"
logo: "https://github.com/clizarraga-UAD7/DataScienceLab/raw/main/images/UADLSquareLogo.png?raw=true"
jupyter: python3
resource_files:
- slides.scss
---

## Objective {.smaller}

::: fragment
Provide a theoretical foundation for deep learning, focusing on the core concepts of neural networks, activation functions, and training strategies.
:::

::: fragment
> No code today ðŸ™‚
:::

## Overview of topics

::: incremental
1.  AI vs. Machine Learning vs. Deep Learning
2.  Intro to Deep Learning
    1.  Structure
    2.  Backpropagation
    3.  Architectures
    4.  Challenges
3.  Summary
:::

# Overview of Artificial Intelligence

## You mean ChatGPT?

::: fragment
![](images/ChatGPT-Logo.png){fig-align="center" width="1000"}
:::

## Artificial Intelligence

![](images/ai-02.png){fig-align="center"}

## Artificial Intelligence

![](images/ai-1.png){fig-align="center"}

## Artificial Intelligence

![](images/ai-2.png){fig-align="center"}

## Artificial Intelligence

![](images/ai-4-01.png){fig-align="center"}

## Artificial Intelligence

![](images/ai-5.png){fig-align="center"}

## Artificial Intelligence

![](images/ai-6.png){fig-align="center"}

## Artificial Intelligence

![](images/ai-7.png){fig-align="center"}

## **First, machine learning**

![](images/ml-01.png){fig-align="center" width="888"}

## Why machine learning in health sciences? {.smaller}

Machine learning has played a very important role in solving problems in:

::: incremental
-   Medical imaging and diagnostics

-   Health economics and predictive healthcare

-   Biomedical research and drug discovery

-   Medical devices and robotics

-   Clinical natural language processing
:::

## What is machine learning? {.smaller}

![](images/ml-workflow.png){fig-align="center"}

::: fragment
Use **machine learning** for [complex tasks]{.underline} with [big data]{.underline} and [many variables]{.underline} when the [underlying formula or equation is unknown]{.underline}
:::

## How does machine learning work? {.smaller}

![](images/ml-types-01.png){fig-align="center" width="1095"}

::: incremental
-   **Supervised** trains a model on labeled data to predict outputs.

-   **Unsupervised** finds hidden patterns in unlabeled data.
:::

## Now, deep learning

![](images/dl.png){fig-align="center" width="908"}

## Why deep learning in health sciences? {.smaller}

::: incremental
-   DL achieves higher diagnostic accuracy than ever before.

-   It's used in critical healthcare applications like disease detection.

-   Developed in the 1980s, it wasn't widely adopted due to limited labeled medical data and computing power.

-   Today, vast labeled datasets, like millions of medical images, enable high-accuracy training.

-   High-performance GPUs and cloud computing now power deep learning efficiently in healthcare.
:::

## What is deep learning? {.smaller}

![](images/dl-workflow.png){fig-align="center"}

::: fragment
Use **deep learning** for [highly complex tasks]{.underline} with [*vast* amounts of data]{.underline} and [intricate patterns]{.underline} when [traditional methods struggle to define the relationships]{.underline}.
:::

## Machine Learning vs. Deep Learning

![](images/ml-ai-01.png){fig-align="center" width="800"}

## Machine Learning vs. Deep Learning {.smaller}

::: incremental
-   DL is a specific type of machine learning.

-   In ML, features are manually extracted from images to develop a classification model.

-   In DL, relevant features are automatically extracted from images.

-   DL uses "end-to-end learning," where a neural network learns directly from raw data to perform classification.

-   DL algorithms improve with more data, while traditional ML models plateau.
:::

# Intro to Deep Learning (DL)

::: fragment
Finally... ðŸ˜¤
:::

::: fragment
> Still no code...
:::

## Artificial Neural Networks {.smaller}

![](images/dl-brain.jpg){fig-align="center" width="764"}

::: fragment
Common interpretation, at the heart of DL.
:::

## Simple Neural Networks

## Deep Learning Networks

![](images/nn-dl.webp){fig-align="center"}

## DL Components {.smaller}

::: incremental
-   Neurons

-   Layers

    -   Input

    -   Hidden

    -   Output

-   Activation Functions

-   Weights

-   Biases
:::

## DL Components: Neurons {.smaller}

![](images/neuron-01.png){fig-align="center" width="847"}

::: incremental
-   The building blocks of neural networks.

-   Each **neuron** performs a weighted sum of inputs, followed by an **activation function.**
:::

## DL Components: Layers {.smaller}

![](images/dl-structure.png){fig-align="center" width="851"}

::: columns
::: {.column width="33.33%"}
![](images/input-02.png){fig-align="center" width="208"}

::: fragment
Data enters here $\rightarrow$
:::
:::

::: {.column width="33.33%"}
![](images/hidden-03.png){fig-align="center" width="214"}

::: fragment
Processed here $\rightarrow$
:::
:::

::: {.column width="33.33%"}
![](images/output-02.png){fig-align="center" width="210"}

::: fragment
Result / prediction
:::
:::
:::

## DL Components: Activation functions {.smaller}

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt

# Define the activation functions
def relu(x):
    return np.maximum(0, x)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, x * alpha)

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

# Generate the data
x = np.linspace(-10, 10, 400)
```

::: panel-tabset
## ReLU

::: columns
::: {.column width="30%"}
```{python}
#| echo: false
plt.figure(figsize=(3, 3))
plt.plot(x, relu(x))
plt.title("ReLU")
plt.show()
```
:::

::: {.column width="70%"}
::: incremental
-   **Formula:** $f(x) = \max(0, x)$

-   **Characteristics:**

    -   Simple and fast to compute.

    -   Keeps positive values, helping the network learn better.

    -   Widely used in hidden layers of neural networks.

-   **Use Case:** Great for deep networks, especially in image recognition.
:::
:::
:::

## Sigmoid

::: columns
::: {.column width="30%"}
```{python}
#| echo: false
plt.figure(figsize=(3, 3))
plt.plot(x, sigmoid(x))
plt.title("Sigmoid")
plt.show()
```
:::

::: {.column width="70%"}
::: incremental
-   **Formula:** $f(x)=\frac{1}{1+e^{âˆ’x}}$â€‹

-   **Characteristics:**

    -   Produces values between 0 and 1, ideal for binary decisions.

    -   Can slow down learning for extreme inputs (very high or low values).

-   **Use Case:** Often used in the final layer for binary classification tasks.
:::
:::
:::

## Softmax

::: columns
::: {.column width="30%"}
```{python}
#| echo: false
plt.figure(figsize=(3, 3))
plt.plot(x, softmax(x))
plt.title("Softmax")
plt.show()
```
:::

::: {.column width="70%"}
::: incremental
-   **Formula:** $f(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$â€‹â€‹

-   **Characteristics:**

    -   Turns outputs into probabilities that sum to 1.

    -   Used for multi-class classification problems.

-   **Use Case:** Essential when there are multiple possible outcomes, like in image classification with many categories.
:::
:::
:::
:::

::: fragment
[How do I choose?](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)
:::

## Activation functions: so what? {.smaller}

**Importance in Neural Networks:**

::: incremental
-   **Non-Linearity:** Activation functions enable networks to tackle complex tasks, like image and speech recognition.

-   **Layer-wise Learning:** They allow each layer to learn different features of the data.

-   **Training Dynamics:** The activation function impacts the network's learning efficiency and speed.
:::

**In Medical Applications:**

::: incremental
-   Key towards DL accurately analyzing and predicting medical data, such as diagnosing diseases or forecasting patient outcomes, by capturing complex biological relationships.
:::

## DL Components: Weights {.smaller}

![](images/weights.png){fig-align="center" width="1278"}

## DL Components: Weights {.smaller}

![](images/weights-1.png){fig-align="center"}

::: incremental
-   **Weights** transform input data in the network's hidden layers.

-   Each **weight** indicates the connection strength between nodes.

-   Initially [small random values]{.underline} (see [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent))
:::

## DL Components: Biases

![](images/bias.png){fig-align="center"}

## DL Components: Biases

::: incremental
-   Biases are extra parameters that shift activation functions.

-   They allow neurons to activate even with zero input.

-   Biases help the model fit data more effectively.
:::

# Batches & Epochs

## Training progression

![](images/batches-epoch.png){fig-align="center"}

## Training progression

![](images/batches-epoch-2.png){fig-align="center"}

## Training progression

![](images/batches-epoch-3.png){fig-align="center"}

## Training progression

![](images/batches-epoch-4.png){fig-align="center"}

## Sample vs. Batches vs. Epoches

$\text{batches per epoch} = \frac{\text{dataset size}}{\text{batch size}}$

::: incremental
-   A **sample** is 1 row of data

-   **Batch size** is the number of samples processed before updating the model

-   The number of **epochs** is the number of complete passes through the training dataset.
:::

## Training progression

![](images/batches-epoch-4.png){fig-align="center"}

::: fragment
This was one pass through the data
:::

## Bad output {.smaller}

Let's say your model output is off target:

::: fragment
![](images/bad-output.png){fig-align="center"}
:::

::: fragment
First, how can we tell?
:::

## Cost function {.smaller}

> A cost function is a mathematical function that calculates the difference between the target actual values (ground truth) and the values predicted by the model.

::: incremental
-   Functionally similar to model evaluation

-   e.g., Logistic Regression (outputs 0, 1):
:::

::: fragment
$$ J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]$$
:::

::: fragment
Yikes... ðŸ˜£
:::

## Bad output {.smaller}

Let's say your model output is off target:

::: fragment
![](images/bad-output.png){fig-align="center"}
:::

::: fragment
What do you do now?..
:::

# Intro to Backpropogation

## Backpropogation

![](images/bp-1-01.png){fig-align="center"}

## Backpropogation

![](images/bp-2-01.png){fig-align="center"}

## Backpropogation

![](images/bp-3.png){fig-align="center"}

## Backpropogation: What is it? {.smaller}

-   A method for updating model weights based on output errors.

-   Helps the network learn by correcting mistakes.

-   Interactively reduces the **cost function** after each **Epoch**

## Backpropogation: Optimization {.smaller}

> **Optimization** refers to the task of minimizing/maximizing an objective function $f(x)$ parameterized by $x$.

**Goals:**

::: incremental
-   Find the global minimum of the objective function, possible if it's convex, meaning any local minimum is also the global minimum.

-   Find the lowest value in the local area of the objective function, typical when the function isn't convex, as in most deep learning problems.
:::

::: fragment
**Example in Health Sciences**: Training models to accurately diagnose diseases from medical images by iteratively improving predictions.
:::

## Backpropogation: How?? {.smaller}

::: columns
::: {.column width="50%"}
**Gradient Descent**

![](images/gradient-descent.png){fig-align="center" width="685"}
:::

::: {.column width="50%"}
::: incremental
-   Minimizes a function by moving towards the steepest descent.

-   It updates parameters using the gradient of the cost function.

-   The learning rate controls step size; balance is key.

-   Continues until reaching a minimum.
:::
:::
:::

# Neural Network Architectures

## Feed forward {.smaller}

![](images/ff.png){fig-align="center" width="846"}

::: incremental
-   Simple, one-way flow of information.

-   Commonly used for tasks like classification and regression.
:::

## Convolutional Neural Networks (CNNs) {.smaller}

![](images/cnn-1.png){fig-align="center" width="960"}

::: {.incremental .small}
-   Designed for image processing.

-   Extracts hierarchical features from images.

-   **Health Sciences Application:** Identifying tumors in radiology images.
:::

## Recurrent Neural Networks (RNNs) {.smaller}

![](images/rnn-01.png){fig-align="center" width="660"}

::: {.incremental .small}
-   Handles sequence data, like time series or language.

-   Captures dependencies over time.

-   **Health Sciences Application:** Predicting patient outcomes over time.
:::

## When to use each architecture {.smaller}

::: fragment
**Feedforward Networks:**

::: incremental
-   Best for tabular data and basic classification tasks.
:::
:::

::: fragment
**CNNs:**

::: incremental
-   Ideal for image and video analysis.

-   **Health Sciences Example:** Detecting skin cancer from images.
:::
:::

::: fragment
**RNNs:**

::: incremental
-   Suited for time-dependent data, such as patient monitoring.

-   **Health Sciences Example:** Forecasting disease progression.
:::
:::

## Challenges {.smaller}

::: {.fragment .small}
**Overfitting:**

::: incremental
-   When a model performs well on training data but fails to generalize to new data.

-   **Solution:** Regularization techniques like [dropout](https://www.geeksforgeeks.org/dropout-regularization-in-deep-learning/) and [data augmentation](https://en.wikipedia.org/wiki/Data_augmentation).
:::
:::

::: {.fragment .small}
**Vanishing Gradients:**

::: incremental
-   Gradients diminish as they are propagated back, slowing learning.

-   **Solution:** Activation functions like [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) and proper weight initialization.
:::
:::

::: {.fragment .small}
**Regularization Techniques:**

::: incremental
-   **Dropout:** Randomly deactivating neurons during training to prevent overfitting.

-   **L2 Regularization:** Penalizes large weights to simplify the model.
:::
:::

# Summary

## Summary {.smaller}

::: columns
::: {.column width="50%"}
![](images/ai-healthcare.webp){fig-align="center" width="600"}
:::

::: {.column width="50%"}
::: incremental
-   Revolutionizing diagnostics and personalized treatments.

-   Success depends on understanding neural networks, architectures, and overcoming challenges.
:::
:::
:::

# Thank You ðŸ˜Š {style="text-align: center;"}
