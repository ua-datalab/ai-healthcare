{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Sequence Modeling with PyTorch (LSTM)\n",
    "### Predict ICU Stay Length based on lab results using LSTM\n",
    "\n",
    "Updated 09/26/2024 G. Chism, U of A InfoSci + DataLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required libraries\n",
    "\n",
    "For this case we will import _PyTorch_, _sklearn_, _pandas_, and _numpy_.\n",
    "\n",
    "**To execute code Notebook cells:** Press _SHIFT+ENTER_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q torch\n",
    "#!pip install -q scikit-learn\n",
    "#!pip install -q pandas\n",
    "#!pip install -q numpy\n",
    "#!pip install watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's best practice to have all of the libraries loaded at the top of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import specific classes from PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Import preprocessing from Scikit-Learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Import pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have GPUs available (hint, we probably won't...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing:\n",
    "\n",
    "### Converting Data Types:\n",
    "\n",
    "`pd.to_numeric()` converts columns like los and valuenum to numeric types, coercing any errors (e.g., invalid strings) to NaN.\n",
    "`pd.to_datetime()` ensures that the charttime and dob columns are properly treated as datetime objects for time-series modeling.\n",
    "\n",
    "### Handling Missing Values:\n",
    "\n",
    "After converting the data types, we check for missing values and handle them (in this case, dropping rows with missing values in critical columns like `los` and `valuenum`).\n",
    "\n",
    "\n",
    "### Check Data Info and Head:\n",
    "\n",
    "This ensures that the data is now clean and ready for modeling, with no incorrect data types or missing values in critical columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the data is already preprocessed via mimic-iii-demo-subset.py and saved as mimic_data.csv\n",
    "mimic_data = pd.read_csv('data/mimic_data.csv')\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "mimic_data['los'] = pd.to_numeric(mimic_data['los'], errors='coerce')  # Convert los to numeric, coercing errors to NaN\n",
    "mimic_data['valuenum'] = pd.to_numeric(mimic_data['valuenum'], errors='coerce')  # Convert valuenum to numeric\n",
    "mimic_data['charttime'] = pd.to_datetime(mimic_data['charttime'], errors='coerce')  # Convert charttime to datetime\n",
    "mimic_data['dob'] = pd.to_datetime(mimic_data['dob'], errors='coerce')  # Convert dob to datetime\n",
    "\n",
    "# Check for any remaining non-numeric or missing data\n",
    "print(mimic_data.info())\n",
    "\n",
    "# Handle missing values (example: drop rows with missing valuenum or los)\n",
    "clean_data = mimic_data.dropna(subset=['los', 'valuenum'])\n",
    "\n",
    "print(clean_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Data by Patients and Time\n",
    "Sort the data by patient and charttime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.sort_values(['subject_id', 'charttime'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sequences for LSTM Input\n",
    "\n",
    "- **Why?** LSTMs require sequential input. We convert each patient’s lab results over time into sequences of defined length (e.g., 10 timesteps) to feed into the LSTM.\n",
    "- **Normalization**: Scaling the lab values ensures that all features contribute equally during training, improving convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (Glucose, Creatinine, Hemoglobin)\n",
    "features = ['valuenum']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "clean_data[features] = scaler.fit_transform(clean_data[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (features): (29086, 10, 1)\n",
      "Shape of y (target): (29086,)\n"
     ]
    }
   ],
   "source": [
    "# Create sequences for LSTM input\n",
    "def create_sequences(df, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for _, group in df.groupby('subject_id'):\n",
    "        values = group[features].values\n",
    "        if len(values) >= sequence_length:\n",
    "            for i in range(len(values) - sequence_length + 1):\n",
    "                sequences.append(values[i:i+sequence_length])\n",
    "                targets.append(group['los'].values[0])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Define sequence length\n",
    "sequence_length = 10\n",
    "X, y = create_sequences(clean_data, sequence_length)\n",
    "\n",
    "print(f'Shape of X (features): {X.shape}')\n",
    "print(f'Shape of y (target): {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare DataLoader:\n",
    "\n",
    "**Why?** PyTorch models require data to be loaded in batches for efficient training. We create a DataLoader to shuffle and batch the sequence data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM Model:\n",
    "\n",
    "**Why?** We define an LSTM with hidden layers and output a single value (ICU stay length). The model will learn to predict ICU stay based on lab sequences.\n",
    "\n",
    "**LSTM**: Uses internal memory to capture long-term dependencies in sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(device)\n",
    "        c_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "# Hyperparameters\n",
    "input_size = len(features)\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop:\n",
    "\n",
    "**Why?** The training loop iterates over batches, performing forward and backward passes. We minimize the error between predicted ICU stays and actual stays using MSE loss and backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 65.0352\n",
      "Epoch 2/20, Loss: 64.2986\n",
      "Epoch 3/20, Loss: 64.3002\n",
      "Epoch 4/20, Loss: 64.2913\n",
      "Epoch 5/20, Loss: 64.3192\n",
      "Epoch 6/20, Loss: 64.3026\n",
      "Epoch 7/20, Loss: 64.2948\n",
      "Epoch 8/20, Loss: 64.3041\n",
      "Epoch 9/20, Loss: 64.2893\n",
      "Epoch 10/20, Loss: 64.3080\n",
      "Epoch 11/20, Loss: 64.3023\n",
      "Epoch 12/20, Loss: 64.2875\n",
      "Epoch 13/20, Loss: 64.2964\n",
      "Epoch 14/20, Loss: 64.2938\n",
      "Epoch 15/20, Loss: 64.2900\n",
      "Epoch 16/20, Loss: 64.2941\n",
      "Epoch 17/20, Loss: 64.3037\n",
      "Epoch 18/20, Loss: 64.2765\n",
      "Epoch 19/20, Loss: 64.2948\n",
      "Epoch 20/20, Loss: 64.2930\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation:\n",
    "\n",
    "**Why?** After training, we evaluate the model’s performance using Mean Absolute Error (MAE), which tells us how close the predicted ICU stay lengths are to the actual lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 5.29\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        predictions.append(outputs.cpu().tolist())\n",
    "        actuals.append(batch_y.cpu().tolist())\n",
    "\n",
    "# Flatten the lists before conversion to numpy arrays\n",
    "predictions = list(itertools.chain.from_iterable(predictions))\n",
    "actuals = list(itertools.chain.from_iterable(actuals))\n",
    "\n",
    "# Convert to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "print(f'Mean Absolute Error: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "We can interpret that our model was off by 5.29 days, which is not ideal.\n",
    "\n",
    "### Improvements\n",
    "\n",
    "1. ***Data Quality and Preprocessing***\n",
    "\n",
    "- **Handle Missing Values**: Impute missing data or remove incomplete rows.\n",
    "\n",
    "- **Normalization**: Normalize features to improve convergence.\n",
    "\n",
    "- **Feature Engineering**: Add relevant features (e.g., demographics, comorbidities). Aggregate lab values over time.\n",
    "\n",
    "2. ***Increase Training Data***\n",
    "\n",
    "- Use a larger dataset or augment sequences to provide more data for learning.\n",
    "\n",
    "3. ***Model Architecture***\n",
    "\n",
    "- **Increase Complexity**: Add more LSTM layers or neurons.\n",
    "\n",
    "- **Bi-Directional LSTM**: Learn from both past and future sequences.\n",
    "\n",
    "- **Attention Mechanism**: Help focus on important parts of the sequence.\n",
    "\n",
    "- **Dropout Layers**: Prevent overfitting and improve generalization.\n",
    "\n",
    "4. ***Hyperparameter Tuning***\n",
    "\n",
    "- Experiment with learning rates, optimizers, batch size, hidden size, and sequence length.\n",
    "\n",
    "5. ***Regularization***\n",
    "\n",
    "- **Dropout**: Randomly drop neurons during training to avoid overfitting.\n",
    "\n",
    "- **L2 Regularization**: Penalize large weights.\n",
    "\n",
    "6. ***Training Strategies***\n",
    "\n",
    "- **Early Stopping**: Stop training when validation performance stops improving.\n",
    "\n",
    "- **Learning Rate Scheduling**: Reduce learning rate during training to improve convergence.\n",
    "\n",
    "- **Cross-Validation**: Ensure model generalizes well.\n",
    "\n",
    "7. ***Ensemble Learning***\n",
    "\n",
    "- Combine multiple models (e.g., LSTMs, GRUs) to improve robustness.\n",
    "\n",
    "8. ***Domain Knowledge***\n",
    "\n",
    "- Use domain expertise for feature selection and define custom loss functions to reflect real-world costs.\n",
    "\n",
    "9. ***Transformer-Based Models***\n",
    "- Use transformers or hybrid models to capture long-range dependencies better.\n",
    "\n",
    "10. ***Target Refinement***\n",
    "\n",
    "- Refine labeling of the target variable (ICU stay length) and consider modeling different types of stays separately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
